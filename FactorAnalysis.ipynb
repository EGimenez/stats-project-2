{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"whiteboard.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that for each individual, we observe $m$ different test $y$ and that there are $p$ anobserved realizations $z$. In total, we have $n$ different individuals. This variables are related as follows:\n",
    "\n",
    "- $ Y|Z \\sim N\\left(\\sideset{^{CZ}}{}\\mu + \\sideset{^{CZ}}{}{L}, \\sideset{^{CZ}}{}\\Sigma \\right) $  \n",
    "- $Z \\sim N\\left(0, 1 \\right)$ \n",
    "\n",
    "where:\n",
    "- $Cov(z_i, z_j) =\\delta_{i,j}$. \n",
    "\n",
    "We will also assume that $\\sideset{^{CZ}}{}\\Sigma$ is a diagonal matrix.\n",
    "\n",
    "The goal is, given $n$ observations of $Y$ $i.i.d$, calculate  $\\Theta = \\left \\{\\sideset{^{CZ}}{}\\mu, \\sideset{^{CZ}}{}L, \\sideset{^{CZ}}{}\\Sigma \\right \\}$  such that:\n",
    "\n",
    "$$ \\Theta = argmax \\left \\{ P \\left(Y_1, ... , Y_n | \\Theta \\right) \\right \\} $$\n",
    "\n",
    "The calculation strategy is as follows:\n",
    "\n",
    "- For a given candidate $\\Theta = \\left \\{\\sideset{^{CZ}}{}\\mu, \\sideset{^{CZ}}{}L, \\sideset{^{CZ}}{}\\Sigma \\right \\}$ we calculate the joined distribution of $(Z, Y)\\sim N\\left \\{\\sideset{^{J}}{}\\mu, \\sideset{^{J}}{}\\Sigma \\right \\}$\n",
    "\n",
    "- Next, we must calculate the posterior probability distribution of $Z|Y$ i.e. $Z \\sim N \\left(\\sideset{^{CY}}{}\\mu, \\sideset{^{CY}}{}\\Sigma \\right )$\n",
    "\n",
    "- At this point we will be able to apply the EM algorithm. i.e. calculate:\n",
    "\n",
    "$$ Q \\left( \\Theta_{new}, \\Theta_{old} \\right) = E_{Z|Y,\\Theta_{old}} \\left[ \\log P\\left( Z, Y | \\Theta_{new} \\right)  \\right]$$\n",
    "\n",
    "The reader will have noticed that we are using a top left script notation to specify at which point of the calculation we are, __CZ__ for conditioned on Z, __J__ for join distribution and __CY__ for conditioned on Y.\n",
    "\n",
    "The following sections derived the calculation of $\\sideset{^{J}}{}P$ and $\\sideset{^{CY}}{}P$. We then present the implementation of the calculation framework and finally we show the result of randomly generated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of the join distribution\n",
    "\n",
    "Since Y|Z is a multivariated normal distribution, Z is also a multivariated normal distribution and the relation between Y and Z is linear, the join distribution is also in multivariate normal distribution. So to calculate the join distribution, all we need to do is calculate $\\sideset{^{J}}{} \\mu$ and  $\\sideset{^{J}}{} \\Sigma$.\n",
    "\n",
    "We will define:\n",
    "\n",
    "$$ X = \\begin{pmatrix}\n",
    "Z \\\\ Y\n",
    "\\end{pmatrix}\n",
    "$$ \n",
    "\n",
    "### Calculation of $\\sideset{^{J}}{} \\mu$\n",
    "Since $Z \\sim N\\left(0, 1 \\right)$ it's easy to see that:\n",
    "\n",
    "$$ \\sideset{^{J}}{} \\mu = \\begin{pmatrix}\n",
    "0_p \\\\ \n",
    "\\sideset{^{CZ}}{} \\mu\n",
    "\\end{pmatrix} $$ \n",
    "\n",
    "Where $0_p$ is a vector of dimension p with all its elements equal to 0.\n",
    "\n",
    "### Calculation of $\\sideset{^{J}}{} \\Sigma $\n",
    "\n",
    "Given that $y_i = \\mu_i + \\sum_k l_{i, k}z_k + \\sigma_i * \\epsilon_i$ all we need to do is calculate $Cov(x_i, x_j)$.\n",
    "\n",
    "- $ Cov(z_i, z_j) = \\delta_{i, j} $\n",
    "- $ Cov(y_i, z_j) = l_{i,j} $\n",
    "- $ Cov(y_i, y_i) = \\sum_k l_{i, k}^2 + \\sigma_i^2$\n",
    "- $ Cov(y_i, y_j) = \\sum_k l_{i, k} l_{j, k} $\n",
    "\n",
    "Where $\\delta_{i,j}$ is the kronecker delta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations of the probability distribution of Z conditioned on Y\n",
    "\n",
    "Let  \n",
    "$$\\sideset{^{J}}{} \\Sigma = \\begin{pmatrix} \n",
    "                    \\sideset{^{J}}{} \\Sigma_{1, 1} & \\sideset{^{J}}{} \\Sigma_{1, 2} \\\\\n",
    "                    \\sideset{^{J}}{} \\Sigma_{2, 1} & \\sideset{^{J}}{} \\Sigma_{2, 2}\n",
    "                    \\end{pmatrix} \\text{ with sizes }\n",
    "                    \\begin{pmatrix} \n",
    "                    p \\times p & p \\times m \\\\\n",
    "                    m \\times p & m \\times m\n",
    "                    \\end{pmatrix}\n",
    "                    $$\n",
    "\n",
    "### Calculation of $\\sideset{^{CY}}{} \\mu$\n",
    "\n",
    "<center>\n",
    "    $\\sideset{^{CY}}{}\\mu =\\sideset{^{J}}{}\\Sigma_{1, 2} \n",
    "\\sideset{^{J}}{} \\Sigma_{2, 2}^{-1} (y - \\sideset{^{CZ}}{}\\mu)$ \n",
    "</center>\n",
    "\n",
    "Since $Z \\sim N(0, 1)$\n",
    "\n",
    "### Calculation of $\\sideset{^{CY}}{} \\Sigma$\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "<center>\n",
    "    $\\sideset{^{CY}}{} \\Sigma =\\sideset{^{J}}{} \\Sigma_{1, 1} -\\sideset{^{J}}{}\\Sigma_{1, 2} \n",
    "\\sideset{^{J}}{} \\Sigma_{2, 2}^{-1} \\sideset{^{J}}{} \\Sigma_{2, 1}$ \n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of $ Q \\left( \\Theta_{new}, \\Theta_{old} \\right) $ \n",
    "\n",
    "In orther to compute $ Q \\left( \\Theta_{new}, \\Theta_{old} \\right) $ i.e. $ = E_{Z|Y,\\Theta_{old}} \\left[ \\log P\\left( Z, Y | \\Theta_{new} \\right)  \\right]$ we will right the former expresion as an integral over $Z$ specifying the density function.\n",
    "\n",
    "That is: \n",
    "<center>\n",
    "\n",
    "$ Q \\left( \\Theta_{new}, \\Theta_{old} \\right)\n",
    "\\propto \n",
    "\\int_{Z} \\left (\n",
    "-\\frac{1}{2}(x - \\sideset{^{J}}{}\\mu_{new})'\n",
    "\\sideset{^{J}}{} \\Sigma_{new}^{-1}\n",
    "(x - \\sideset{^{J}}{}\\mu_{new}) \n",
    "- \\frac{1}{2} \\log |\\sideset{^{J}}{} \\Sigma_{new}| \n",
    "\\right) \n",
    "\\frac{exp\n",
    "\\left (\n",
    "-\\frac{1}{2}(x - \\sideset{^{CY}}{}\\mu_{old})'\n",
    "\\sideset{^{CY}}{} \\Sigma_{old}^{-1}\n",
    "(x - \\sideset{^{CY}}{}\\mu_{old}) \n",
    "\\right) \n",
    "}\n",
    "{\\sqrt{|\\sideset{^{CY}}{} \\Sigma_{old}|}} dz$\n",
    "    </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this integral is fairly simple, because what we are integrating over $Z$ is a quadratic form on $X = (Z, Y)$ minus a function, $-\\frac{1}{2} \\log |\\sideset{^{J}}{} \\Sigma_{new}|$. This function on $\\Theta_{new}$ is a constant when integrated over $Z$.\n",
    "\n",
    "Now, the quadratic form has the following cases, $z_i$ vs $z_i$, $z_i$ vs $z_j$, $y_i$ vs $z_j$ and $y_i$ vs $y_j$\n",
    "\n",
    "### $z_i$ vs $z_i$:\n",
    "\n",
    "\n",
    "$ \\begin{align}\n",
    "E_{Z|Y,\\Theta_{old}}[^{J}\\Sigma^{-1}_{new, i, i}*(z_i - ^{J}\\mu_{new, i})^2] \n",
    "&= E_{Z|Y,\\Theta_{old}}[^{J}\\Sigma^{-1}_{new, i, i}*(z_i - {^{CY}\\mu_{old, i}} + {^{CY}\\mu_{old, i}} - ^{J}\\mu_{new, i})^2] \\\\\n",
    "&= E_{Z|Y,\\Theta_{old}}[^{J}\\Sigma^{-1}_{new, i, i}*(z_i - {^{CY}\\mu_{old, i}} + {^{CY}\\mu_{old, i}})^2] \\\\\n",
    "&= ^{J}\\Sigma^{-1}_{new, i, i}*E_{Z|Y,\\Theta_{old}}[(z_i - {^{CY}\\mu_{old, i}} + {^{CY}\\mu_{old, i}})^2] \\\\\n",
    "&= ^{J}\\Sigma^{-1}_{new, i, i}*E_{Z|Y,\\Theta_{old}}[(z_i - {^{CY}\\mu_{old, i}})^2 + 2(z_i - {^{CY}\\mu_{old, i}}) + {^{CY}\\mu_{old, i}^2}] \\\\\n",
    "&= ^{J}\\Sigma^{-1}_{new, i, i}*E_{Z|Y,\\Theta_{old}}[(z_i - {^{CY}\\mu_{old, i}})^2 + ^{CY}\\mu_{old, i}^2] \\\\\n",
    "&= ^{J}\\Sigma^{-1}_{new, i, i}*(^{CY}\\Sigma_{old, i, i} + {^{CY}\\mu_{old, i}^2}) \n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "\n",
    "Since $ \\sideset{^{J}}{} \\mu_{new, i} = 0$ \n",
    "\n",
    "\n",
    "###  $z_i$ vs $z_j$:\n",
    "\n",
    "$ \\begin{align}\n",
    "E_{Z|Y,\\Theta_{old}}[^{J}\\Sigma^{-1}_{new, i, j}(z_i -  ^{J}\\mu_{new, i})(z_j - ^{J}\\mu_{new, j})] \n",
    "&=2 ^{J}\\Sigma_{new, i, j}^{-1} E_{Z|Y,\\Theta_{old}}[(z_i -  {^{J}\\mu_{new, i}})(z_j - ^{J}\\mu_{new, j})] \\\\\n",
    "&=2 ^{J}\\Sigma_{new, i, j}^{-1} E_{Z|Y,\\Theta_{old}}[(z_i - {^{CY}\\mu_{old, i}} +  {^{CY}\\mu_{old, i}}-  ^{J}\\mu_{new, i})\n",
    "(z_j - {^{CY}\\mu_{old, j}}+ {^{CY}\\mu_{old, j}}- {^{J}\\mu_{new, j}})] \\\\\n",
    "&=2 ^{J}\\Sigma_{new, i, j}^{-1} E_{Z|Y,\\Theta_{old}}[(z_i - {^{CY}\\mu_{old, i}})(z_j - {^{CY}\\mu_{old, j}})+({^{CY}\\mu_{old, i}}-  ^{J}\\mu_{new, i})({^{CY}\\mu_{old, j}}- {^{J}\\mu_{new, j}})] \\\\ \n",
    "&=2 ^{J}\\Sigma_{new, i, j}^{-1} (^{CY}\\Sigma_{old, i, j} + ({^{J}\\mu_{old, i}}-  ^{J}\\mu_{new, i})({^{CY}\\mu_{old, j}}- {^{J}\\mu_{new, j}})) \\\\\n",
    "&=2 ^{J}\\Sigma_{new, i, j}^{-1} (^{CY}\\Sigma_{old, i, j} + {^{CY}\\mu_{old, i}}{^{CY}\\mu_{old, j}}) \n",
    "\\end{align}$\n",
    "\n",
    "Again since  $\\sideset{^{J}}{} \\mu_{new, i} = 0$.\n",
    "\n",
    "### $y_i$ vs $z_j$:\n",
    "\n",
    "$\\begin{align}\n",
    "E_{Z|Y,\\Theta_{old}}[{^{J}\\Sigma^{-1}_{new, i, j}}(z_i - {^{J}\\mu_{new, i}})(y_j - {^{J}\\mu_{new, j}})] \n",
    "&=2{^{J}\\Sigma^{-1}_{new, i, j}}E_{Z|Y,\\Theta_{old}}[(z_i - {^{J}\\mu_{new, i}})(y_j - {^{J}\\mu_{new, j}})] \\\\ \n",
    "&=2{^{J}\\Sigma^{-1}_{new, i, j}}(y_j - {^{J}\\mu_{new, j}})E_{Z|Y,\\Theta_{old}}[(z_i - {^{J}\\mu_{new, i}})] \\\\ \n",
    "&=2{^{J}\\Sigma^{-1}_{new, i, j}}(y_j - {^{J}\\mu_{new, j}})^{CY}\\mu_{old, i}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Again since  $\\sideset{^{J}}{} \\mu_{new, i} = 0$.\n",
    "\n",
    "### $y_i$ vs $y_j$:\n",
    "\n",
    "$E_{Z|Y,\\Theta_{old}}[\\sideset{^{J}}{}  \\Sigma_{new, i, j}^{-1}(y_i - \\sideset{^{J}}{} \\mu_{new, i})(y_j - \\sideset{^{J}}{} \\mu_{new, j})] = 2\\sideset{^{J}}{}  \\Sigma_{new, i, j}^{-1}(y_i - \\sideset{^{J}}{} \\mu_{new, i})(y_j - \\sideset{^{J}}{} \\mu_{new, j})$\n",
    "\n",
    "\n",
    "### $y_i$ vs $y_i$:\n",
    "\n",
    "$E_{Z|Y,\\Theta_{old}}[\\sideset{^{J}}{}  \\Sigma_{new, i, i}^{-1}(y_i - \\sideset{^{J}}{} \\mu_{new, i})(y_i - \\sideset{^{J}}{} \\mu_{new, i})] = \\sideset{^{J}}{}  \\Sigma_{new, i, i}^{-1}(y_i - \\sideset{^{J}}{} \\mu_{new, i})^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementatios has been divided in three sections:\n",
    "- __Factor Analysis Classes:__ Here we implement two classes. \n",
    "    - __MultivariateDistribution:__ Encapsulating all the machinery for probability distributions calculations i.e. Condition on Z, Join and Condition on Y.\n",
    "    - __FactorAnalysisModel:__ Encapsulation the calculations of $Q(\\theta_{new}, \\theta_{old})$\n",
    "- __Data Generation Process:__ Given ${^{CZ}\\mu}$, ${^{CZ}L}$ and ${^{CZ}\\Sigma}$ generates n samples of $Y$\n",
    "- __Optimization Helper Functions:__ \n",
    "    - __<code>break_params:</code>__ Function for translating the <code>np.array</code> that the scipy minimizer is able to deal with to three different parmeters ${^{CZ}\\mu}$, ${^{CZ}L}$ and ${^{CZ}\\Sigma}$ that our class is able to manage. \n",
    "    - __<code>get_f:</code>__ Function for encapsulation all the object oriented programming into a function that the scipy minimizer will deal with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateDistribution(object):\n",
    "\tdef __init__(self, Y):\n",
    "\t\tself.Y = Y\n",
    "\t\tself._CZ_MU = None\n",
    "\t\tself._CZ_LF = None\n",
    "\t\tself._CZ_SIG = None\n",
    "\t\tself._J_SIG = None\n",
    "\t\tself._CY_SIG = None\n",
    "\t\tself._J_sig_inv = None\n",
    "\t\tself._J_sig_log_det = None\n",
    "\n",
    "\tdef set_theta(self, MU, LF, SIG):\n",
    "\t\tself._CZ_MU = MU\n",
    "\t\tself._CZ_LF = LF\n",
    "\t\tself._CZ_SIG = SIG\n",
    "\t\tself._J_SIG = None\n",
    "\t\tself._CY_SIG = None\n",
    "\t\tself._J_sig_inv = None\n",
    "\t\tself._J_sig_log_det = None\n",
    "\n",
    "\tdef get_X(self):\n",
    "\t\tn = self.Y.shape[1]\n",
    "\t\tp = self.get_p()\n",
    "\n",
    "\t\tz = np.zeros((p, n))\n",
    "\n",
    "\t\treturn np.concatenate((z, self.Y))\n",
    "\n",
    "\tdef get_J_mu(self):\n",
    "\t\tp = self.get_p()\n",
    "\t\tz = np.zeros((p, 1))\n",
    "\t\treturn np.concatenate((z, self._CZ_MU), axis=0)\n",
    "\n",
    "\tdef get_J_sig(self):\n",
    "\t\t# return np.array [(p + m) x (p + m)]\n",
    "\t\tp = self.get_p()\n",
    "\t\tm = self.get_m()\n",
    "\n",
    "\t\tif self._J_SIG is None:\n",
    "\t\t\tsig = np.zeros((p+m, p+m))\n",
    "\n",
    "\t\t\tfor i in range(p+m):\n",
    "\t\t\t\tfor j in range(p+m):\n",
    "\t\t\t\t\tif i < p:\n",
    "\t\t\t\t\t\tif j < p:\n",
    "\t\t\t\t\t\t\tif i == j:\n",
    "\t\t\t\t\t\t\t\tsig[i, i] = 1\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tsig[i, j] = self._CZ_LF[j-p, i] # ??????\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tif j < p:\n",
    "\t\t\t\t\t\t\tsig[i, j] = self._CZ_LF[i-p, j] # ?????\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tif i == j:\n",
    "\t\t\t\t\t\t\t\tfor k in range(p):\n",
    "\t\t\t\t\t\t\t\t\tsig[i, j] += self._CZ_LF[i-p, k]**2 # ?????\n",
    "\t\t\t\t\t\t\t\tsig[i, j] += self._CZ_SIG[i-p]**2\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tfor k in range(p):\n",
    "\t\t\t\t\t\t\t\t\tsig[i, j] += self._CZ_LF[i-p, k]*self._CZ_LF[j-p, k] # ?????\n",
    "\n",
    "\t\t\tself._J_SIG = sig\n",
    "\n",
    "\t\treturn self._J_SIG\n",
    "\n",
    "\tdef get_CY_mu(self, y):\n",
    "\t\tp = self.get_p()\n",
    "\t\tm = self.get_m()\n",
    "\t\ty = y.reshape((m, 1))\n",
    "\t\tCZ_MU = self._CZ_MU\n",
    "\t\tJ_SIG = self.get_J_sig()\n",
    "\t\tsig_12 = J_SIG[:p, p:]\n",
    "\t\tsig_22 = J_SIG[p:, p:]\n",
    "\t\tsig_22_inv = np.linalg.inv(sig_22)\n",
    "\n",
    "\t\treturn sig_12@sig_22_inv@(y - CZ_MU)\n",
    "\n",
    "\tdef get_J_sig_inv(self):\n",
    "\t\tif self._J_sig_inv is None:\n",
    "\t\t\tself._J_sig_inv = np.linalg.inv(self.get_J_sig())\n",
    "\n",
    "\t\treturn self._J_sig_inv\n",
    "\n",
    "\tdef get_CY_sig(self):\n",
    "\t\t# TODO:\n",
    "\t\t# return np.array [p x p]\n",
    "\t\tp = self.get_p()\n",
    "\t\tm = self.get_m()\n",
    "\n",
    "\t\tif self._CY_SIG is None:\n",
    "\n",
    "\t\t\tJ_sig = self.get_J_sig()\n",
    "\t\t\tsig11 = J_sig[:p, :p]\n",
    "\t\t\tsig12 = J_sig[:p, p:]\n",
    "\t\t\tsig21 = J_sig[p:, :p]\n",
    "\t\t\tsig22 = J_sig[p:, p:]\n",
    "\n",
    "\t\t\tsig = sig11 - sig12 @ np.linalg.inv(sig22) @ sig21\n",
    "\t\t\tself._CY_SIG = sig\n",
    "\n",
    "\t\treturn self._CY_SIG\n",
    "\n",
    "\tdef get_J_sig_log_det(self):\n",
    "\t\tif self._J_sig_log_det is None:\n",
    "\t\t\tself._J_sig_log_det = np.log(np.linalg.det(self.get_J_sig()))\n",
    "\n",
    "\t\treturn self._J_sig_log_det\n",
    "\n",
    "\tdef get_p(self):\n",
    "\t\treturn self._CZ_LF.shape[1]\n",
    "\n",
    "\tdef get_m(self):\n",
    "\t\treturn self._CZ_LF.shape[0]\n",
    "\n",
    "class FactorAnalysisModel(object):\n",
    "\tdef __init__(self, mvd_old, mvd_new):\n",
    "\t\tself.mvd_old = mvd_old\n",
    "\t\tself.mvd_new = mvd_new\n",
    "\n",
    "\tdef set_theta(self, MU, LF, SIG):\n",
    "\t\tself.mvd_new.set_theta(MU, LF, SIG)\n",
    "\n",
    "\tdef q_x(self, x):\n",
    "\t\t# y [m x 1]\n",
    "\t\t# x [(p + m) x 1]\n",
    "\n",
    "\t\tresult = 0\n",
    "\t\tp = self.mvd_old.get_p()\n",
    "\n",
    "\t\tCY_mu_old = self.mvd_old.get_CY_mu(x[p:])\n",
    "\t\tCY_sig_old = self.mvd_old.get_CY_sig()\n",
    "\t\tJ_mu_new = self.mvd_new.get_J_mu()\n",
    "\t\tJ_sig_new_inv = self.mvd_new.get_J_sig_inv()\n",
    "\n",
    "\t\t# -1/2 (x-J_mu_new)' * J_sig_new_inv * (x -J_mu_new) -1/2 log(det(J_sig_new))\n",
    "\t\t# -1/2 log(det(J_sig_new))\n",
    "\t\tresult += self.mvd_new.get_J_sig_log_det()\n",
    "\n",
    "\t\t# -1/2 (x-J_mu_new)' * J_sig_new_inv * (x -J_mu_new)\n",
    "\t\tfor i in range(J_sig_new_inv.shape[0]):\n",
    "\t\t\tfor j in range(J_sig_new_inv.shape[1]):\n",
    "\t\t\t\tif (i < p) and (j < p):\n",
    "\t\t\t\t\t# We are in Z,Z\n",
    "\t\t\t\t\tresult += J_sig_new_inv[i, i] * (CY_sig_old[i, j] + CY_mu_old[i] * CY_mu_old[j])\n",
    "\t\t\t\telif (i >= p) and (j >= p):\n",
    "\t\t\t\t\tresult += J_sig_new_inv[i, j] * (x[i] - J_mu_new[i])*(x[j] - J_mu_new[j])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tif i < j:\n",
    "\t\t\t\t\t\tresult += J_sig_new_inv[i, j] * (x[j] - J_mu_new[j]) * CY_mu_old[i]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tresult += J_sig_new_inv[i, j] * (x[i] - J_mu_new[i]) * CY_mu_old[j]\n",
    "\n",
    "\t\treturn -1/2*result\n",
    "\n",
    "\tdef q(self):\n",
    "\t\tresult = 0\n",
    "\t\tX = self.mvd_old.get_X()\n",
    "\t\tfor i in range(X.shape[1]):\n",
    "\t\t\tresult += self.q_x(X[:, i])\n",
    "\n",
    "\t\treturn result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module is entitle to the generation of data given\n",
    "# MU\n",
    "# LOADING FACTORS\n",
    "# SIGMA\n",
    "\n",
    "def gen_data(MU, LF, SIG, n):\n",
    "\t# TODO\n",
    "\t# Y ~ N(MU + LF*Z, SIG)\n",
    "\t# MU\n",
    "\t# LOADING FACTORS\n",
    "\t# SIGMA\n",
    "\t# n: number of generated Ys\n",
    "\t# return a [m x n] matrix\n",
    "\tm = MU.shape[0]\n",
    "\tp = MU.shape[1]\n",
    "\n",
    "\tepsilons = np.random.normal(0, 1, (m, n))\n",
    "\tepsilons = epsilons*SIG.reshape((m, 1))\n",
    "\n",
    "\tZs = np.random.normal(0, 1, (p, n))\n",
    "\n",
    "\tYs = MU + LF@Zs + epsilons\n",
    "\n",
    "\treturn np.array(Ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_params(theta, y_dim, z_dim):\n",
    "\t# TODO\n",
    "\t# MU <- theta()\n",
    "\t# LF <- theta()\n",
    "\t# SIG <- theta()\n",
    "\n",
    "\tMU = theta[:y_dim].reshape((y_dim, 1))\n",
    "\tLF = theta[y_dim:-y_dim].reshape((y_dim, z_dim))\n",
    "\tSIG = theta[-y_dim:]**2 # + 0.00000001 # This **2 is to avoid restrictions on the optimizer\n",
    "\treturn MU, LF, SIG\n",
    "\n",
    "\n",
    "def get_f(fam, y_dim, z_dim):\n",
    "\n",
    "\tdef f(theta):\n",
    "\t\tMU, LF, SIG = break_params(theta, y_dim, z_dim)\n",
    "\t\tfam.mvd_new.set_theta(MU, LF, SIG)\n",
    "\t\treturn -fam.q()\n",
    "\n",
    "\treturn f\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate the following data set, dim(y)=2, dim(Z)=1\n",
    "\n",
    "    CZ_MU = np.zeros((2, 1))\n",
    "\tCZ_L = np.matrix([[1], [3]])\n",
    "\tCZ_SIG = np.array([1, 1])\n",
    "    \n",
    "Our initial guess is \n",
    "\n",
    "    CZ_MU = np.array([0.5, 0.5])\n",
    "\tCZ_L = np.matrix([[1.5], [3.5]])\n",
    "\tCZ_SIG = np.array([1.5, 1.5])\n",
    "\n",
    "\n",
    "The number of samples is 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2941 0.5989 0.9918 3.355  1.1061 1.4102]\n",
      "[0.1531 0.5045 1.0644 3.0236 0.9985 1.3334]\n",
      "[0.1618 0.3595 1.0774 2.8879 0.9587 1.2957]\n",
      "[0.1227 0.3406 1.0836 2.8516 0.9428 1.2783]\n",
      "[0.1104 0.2617 1.0765 2.8224 0.9353 1.2711]\n",
      "[0.092  0.211  1.0715 2.8078 0.932  1.268 ]\n",
      "[0.077  0.1698 1.0688 2.7985 0.9308 1.2667]\n",
      "[0.0641 0.1373 1.0668 2.792  0.9306 1.2661]\n",
      "[0.0538 0.1104 1.0651 2.7881 0.9305 1.2658]\n",
      "[0.0454 0.0884 1.064  2.7854 0.9305 1.2657]\n",
      "[0.0386 0.0704 1.0633 2.7834 0.9306 1.2657]\n",
      "[0.033  0.0558 1.0626 2.7823 0.9306 1.2657]\n",
      "[0.0284 0.0437 1.0623 2.7815 0.9307 1.2657]\n",
      "[0.0246 0.0337 1.0619 2.7808 0.9307 1.2658]\n",
      "[0.0214 0.0258 1.0618 2.7802 0.9307 1.2657]\n",
      "[0.019  0.0191 1.0615 2.7799 0.9307 1.2657]\n",
      "[0.0169 0.0136 1.0615 2.7795 0.9308 1.2657]\n",
      "[0.0153 0.0092 1.0614 2.7796 0.9307 1.2657]\n",
      "[0.0138 0.0056 1.0614 2.7795 0.9307 1.2657]\n",
      "[1.2500e-02 2.7000e-03 1.0613e+00 2.7792e+00 9.3070e-01 1.2657e+00]\n",
      "[1.1400e-02 2.0000e-03 1.0615e+00 2.7793e+00 9.3070e-01 1.2656e+00]\n",
      "[ 1.1300e-02 -1.2000e-03  1.0614e+00  2.7794e+00  9.3070e-01  1.2657e+00]\n",
      "[ 1.0500e-02 -1.3000e-03  1.0613e+00  2.7793e+00  9.3070e-01  1.2657e+00]\n",
      "[ 1.0100e-02 -2.5000e-03  1.0613e+00  2.7795e+00  9.3080e-01  1.2657e+00]\n",
      "[ 0.01   -0.0045  1.0613  2.7793  0.9308  1.2657]\n",
      "Hey ho lets go\n"
     ]
    }
   ],
   "source": [
    "\ty_dim = 2\n",
    "\tz_dim = 1\n",
    "\tCZ_MU = np.zeros((2, 1))\n",
    "\tCZ_L = np.matrix([[1], [3]])\n",
    "\tCZ_SIG = np.array([1, 1])\n",
    "    \n",
    "\tn = 1000\n",
    "\n",
    "\tY = gen_data(CZ_MU, CZ_L, CZ_SIG, n)\n",
    "\n",
    "\tmvd_old = MultivariateDistribution(Y)\n",
    "\tmvd_new = MultivariateDistribution(Y)\n",
    "\n",
    "\tinit_guess = np.array([0.5, 0.5, 1.5, 3.5, 1.5, 1.5])\n",
    "\tmu_init, l_init, sig_init = break_params(init_guess, y_dim, z_dim)\n",
    "\n",
    "\tmvd_old.set_theta(mu_init, l_init, sig_init)\n",
    "\tmvd_new.set_theta(mu_init, l_init, sig_init)\n",
    "\n",
    "\tfam = FactorAnalysisModel(mvd_old, mvd_new)\n",
    "\n",
    "\tf = get_f(fam, y_dim, z_dim)\n",
    "\n",
    "\tres = minimize(f,\n",
    "\t               init_guess,\n",
    "\t               method='Nelder-Mead',\n",
    "\t               options={'maxiter': 100})\n",
    "\n",
    "\n",
    "\tfor s in range(25):\n",
    "\t\tprint(np.round(res.x, 4))\n",
    "\t\tmu, lf, sig = break_params(res.x, y_dim, z_dim)\n",
    "\t\tfam.mvd_old.set_theta(mu, lf, sig)\n",
    "\t\t#f = get_f(fam, y_dim, z_dim)\n",
    "\t\tres = minimize(f,\n",
    "\t\t               res.x,\n",
    "\t\t               method='Nelder-Mead',\n",
    "\t\t               options={'maxiter': 100})\n",
    "\n",
    "\tmu, lf, sig = break_params(res.x, y_dim, z_dim)\n",
    "\n",
    "\tprint('Hey ho lets go')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that values are recovered.\n",
    "\n",
    "At this point, with the parameters ${^{CZ}\\mu} + {^{CZ}{L}}, {^{CZ}\\Sigma}$ we would be able to assign to each individual an instance of the latent variables by calculating the maximum likelyhood estimator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
